"""
 __author__ = "Mohammed Shehab"
 __copyright__ = "Copyright (c) 2024 Mohammed Shehab"
 __credits__ = ["Mohammed Shehab", "Safwan Omari", "Yaser Jararweh"]
 __license__ = "MIT License"
 __version__ = "1.0.0"
 __maintainer__ = "Mohammed Shehab"
 __email__ = "shihab@live.cn"
 __status__ = "Development"
"""

import matplotlib.pyplot as plt
import numpy as np
from scipy.interpolate import interp1d
from sklearn.metrics import precision_recall_curve, average_precision_score
from modules.DataHandler import DataHandler
import pandas as pd


class VulnerabilityAnalyzer:
    def __init__(self, project_name, data_control):
        """
        Initialize the analyzer with file paths for comparison data.

        :param cppcheck_file: Path to the JSON file labelled by cppCheck
        :param traditional_vszz_file: Path to the JSON file labelled by traditional V-SZZ
        :param advanced_vszz_file: Path to the JSON file labelled by advanced V-SZZ
        """
        self.cppcheck_data = None
        self.traditional_vszz_data = None
        self.advanced_vszz_data = None
        self.hybrid_data = None
        self.project_name = project_name
        # Todo: Change the base_dir to the correct path
        self.data_control = data_control
        # Load and merge the data from all files
        self.comparison_results = self.load_and_merge_data()

        # Filter cppcheck vulnerabilities and create manual labels
        self.generate_manual_labels()

    def load_and_merge_data(self):
        """
        Load and merge data from the three JSON files.

        :param cppcheck_file: JSON file with cppCheck labels
        :param traditional_vszz_file: JSON file with traditional V-SZZ labels
        :param advanced_vszz_file: JSON file with advanced V-SZZ labels
        :return: List of dictionaries with merged data
        """
        self.cppcheck_data = self.data_control.load_json(self.project_name, process_type="commit_labels_cppcheck")
        self.traditional_vszz_data = self.data_control.load_json(self.project_name,
                                                                 process_type=f"commits_introduce_vulnerabilities_traditional")
        self.advanced_vszz_data = self.data_control.load_json(self.project_name,
                                                              process_type=f"commits_introduce_vulnerabilities_advanced")
        # Convert JSON data to DataFrames and merge on 'hash_introduced' as the common key
        cppcheck_df = pd.DataFrame(self.cppcheck_data)
        cppcheck_df = cppcheck_df.drop(columns=['vulnerable_time', 'vulnerabilities'], axis=1)
        cppcheck_df['cppcheck_label'] = 1

        traditional_vszz_df = (pd.DataFrame(self.traditional_vszz_data).
                               rename(columns={'bug_pattern': 'traditional_vszz',
                                               'severity': 'severity_traditional'}))
        traditional_vszz_df = traditional_vszz_df.drop(columns=['hash_fix', 'fixing_time', 'vulnerable_time'], axis=1)
        traditional_vszz_df['traditional_vszz_label'] = 1

        advanced_vszz_df = (pd.DataFrame(self.advanced_vszz_data).
                            rename(columns={'bug_pattern': 'advanced_vszz',
                                            'severity': 'severity_advanced'}))
        advanced_vszz_df = advanced_vszz_df.drop(columns=['hash_fix', 'fixing_time', 'vulnerable_time'], axis=1)
        advanced_vszz_df['advanced_vszz_label'] = 1


        # Perform an outer merge to ensure all commits are included, filling missing values with 0 or empty lists
        merged_df = cppcheck_df.merge(traditional_vszz_df, on="hash_introduced", how="outer")
        merged_df = (merged_df.merge(advanced_vszz_df, on="hash_introduced", how="outer").fillna({
            'cppcheck_label': 0, 'traditional_vszz_label': 0, 'advanced_vszz_label': 0, 'hybrid_vszz_label': 0,
            "traditional_vszz": "", "severity_traditional": "",
            "advanced_vszz": "", "severity_advanced": "",
        }))

        # Convert merged DataFrame back to list of dictionaries
        return merged_df  #.to_dict(orient="records")

    def generate_manual_labels(self):
        """
        Filter cppcheck vulnerabilities to label them as important (1) or unimportant (0) for manual labels.
        """
        non_critical_patterns = {"normalCheckLevelMaxBranches", "uselessCallsSubstr", "postfixOperator",
                                 "passedByValue"}

        # Step 1: Compile a list of hash_introduced with only non-critical vulnerabilities
        non_critical_hashes = [
            entry["hash_introduced"]
            for entry in self.cppcheck_data
            if all(vuln in non_critical_patterns for vuln in entry.get("vulnerabilities", {}))
        ]

        # Step 2: Add a manual_label column to comparison_results DataFrame
        self.comparison_results["manual_label"] = self.comparison_results["hash_introduced"].apply(
            lambda x: 0 if x in non_critical_hashes else 1
        )
        # Update manual_label to 1 for rows with hash_introduced in the intersection
        filtered_advanced = self.comparison_results[self.comparison_results["advanced_vszz_label"] == 1]
        filtered_traditional = self.comparison_results[self.comparison_results["traditional_vszz_label"] == 1]

        intersection_hashes = set(filtered_advanced["hash_introduced"]).intersection(
            set(filtered_traditional["hash_introduced"]))

        self.comparison_results.loc[
            self.comparison_results["hash_introduced"].isin(intersection_hashes), "manual_label"] = 1

        # Count rows where manual_label is set to 0 (non-critical)
        affected_rows = (self.comparison_results["manual_label"] == 0).sum()

        print(f"Number of rows affected: {affected_rows}")

    def calculate_confusion_matrix_metrics(self, export_filename="confusion_matrix_metrics.csv", visualize_pr=False):
        """
        Calculate confusion matrix metrics (precision, recall, accuracy, F1 Score, FPR, FNR) for each label,
        including hybrid results derived from "traditional_vszz_label" and "advanced_vszz_label",
        export the results to a CSV file, and optionally plot Precision-Recall (PR) curves with Average Precision (AP).

        :param export_filename: Name of the CSV file to export the metrics (default: 'confusion_matrix_metrics.csv')
        :param visualize_pr: Whether to plot PR curves and compute Average Precision (default: True)
        :return: A dictionary with metrics for each label compared to manual_label
        """
        results = {}
        labels = ["cppcheck_label", "traditional_vszz_label", "advanced_vszz_label"]

        # Generate hybrid labels
        self.comparison_results["hybrid_union_label"] = (
                (self.comparison_results["traditional_vszz_label"] == 1) |
                (self.comparison_results["advanced_vszz_label"] == 1)
        ).astype(int)

        self.comparison_results["hybrid_intersection_label"] = (
                (self.comparison_results["traditional_vszz_label"] == 1) &
                (self.comparison_results["advanced_vszz_label"] == 1)
        ).astype(int)

        self.comparison_results["hybrid_xor_label"] = (
            (self.comparison_results["traditional_vszz_label"] != self.comparison_results["advanced_vszz_label"])
        ).astype(int)

        labels.extend(["hybrid_union_label", "hybrid_intersection_label", "hybrid_xor_label"])

        manual_label = self.comparison_results["manual_label"].astype(int)
        export_data = []

        if visualize_pr:
            plt.figure(figsize=(10, 8))

        for label in labels:
            label_predictions = self.comparison_results[label].astype(int)

            # Compute confusion matrix components
            TP = ((manual_label == 1) & (label_predictions == 1)).sum()
            FP = ((manual_label == 0) & (label_predictions == 1)).sum()
            FN = ((manual_label == 1) & (label_predictions == 0)).sum()
            TN = ((manual_label == 0) & (label_predictions == 0)).sum()

            # Calculate metrics
            precision = TP / (TP + FP) if (TP + FP) > 0 else 0
            recall = TP / (TP + FN) if (TP + FN) > 0 else 0
            accuracy = (TP + TN) / (TP + FP + FN + TN) if (TP + FP + FN + TN) > 0 else 0
            f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
            fpr = FP / (FP + TN) if (FP + TN) > 0 else 0
            fnr = FN / (FN + TP) if (FN + TP) > 0 else 0

            # Get predictions and simulate probabilities
            label_predictions = self.comparison_results[label].astype(int)
            probabilistic_predictions = label_predictions + np.random.uniform(0, 0.1, size=label_predictions.shape)
            probabilistic_predictions = np.clip(probabilistic_predictions, 0, 1)

            # Compute Precision-Recall curve
            precision_values, recall_values, _ = precision_recall_curve(manual_label, probabilistic_predictions)
            average_precision = average_precision_score(manual_label, probabilistic_predictions)

            # Interpolate for smoother curve
            recall_interp = np.linspace(0, 1, 500)
            precision_interp = interp1d(recall_values, precision_values, kind="linear", fill_value="extrapolate")(recall_interp)

            # Plot PR Curve
            if visualize_pr:
                plt.plot(recall_interp, precision_interp, label=f"{label} (AP = {average_precision:.2f})")

            # Compute Precision-Recall curve and Average Precision
            # precision_values, recall_values, _ = precision_recall_curve(manual_label, label_predictions)
            # average_precision = average_precision_score(manual_label, label_predictions)

            # Print metrics
            print(f"Metrics for {label}:")
            print(f"  Precision: {precision:.2f}")
            print(f"  Recall: {recall:.2f}")
            print(f"  Accuracy: {accuracy:.2f}")
            print(f"  F1 Score: {f1_score:.2f}")
            print(f"  False Positive Rate (FPR): {fpr:.2f}")
            print(f"  False Negative Rate (FNR): {fnr:.2f}")
            print(f"  Average Precision (AP): {average_precision:.2f}")

            # Store results
            results[label] = {
                "precision": precision,
                "recall": recall,
                "accuracy": accuracy,
                "f1_score": f1_score,
                "false_positive_rate": fpr,
                "false_negative_rate": fnr
            }

            export_data.append({
                "Label": label,
                "Precision": precision,
                "Recall": recall,
                "Accuracy": accuracy,
                "F1 Score": f1_score,
                "False Positive Rate (FPR)": fpr,
                "False Negative Rate (FNR)": fnr
            })

            # Plot PR Curve
            # if visualize_pr:
            #     plt.plot(recall_values, precision_values, label=f"{label} (AP = {average_precision:.2f})")

        # Export metrics to CSV
        metrics_df = pd.DataFrame(export_data)
        metrics_df.to_csv(export_filename, index=False)
        print(f"Confusion matrix metrics exported to {export_filename}")

        # Finalize PR Curve
        if visualize_pr:
            plt.xlabel("Recall")
            plt.ylabel("Precision")
            plt.title("Precision-Recall Curve")
            plt.legend(loc="lower left")
            plt.grid(True)
            plt.tight_layout()
            plt.savefig(f"./{self.project_name}/{self.project_name}_precision_recall_curve.png")
            plt.show()

        return results


if __name__ == "__main__":
    project_name = "impala"
    cppcheck_results = f"./{project_name}/{project_name}_commit_labels_cppcheck.json"
    traditional_vszz_results = f"./{project_name}/{project_name}_commits_introduce_vulnerabilities_traditional.json"
    advanced_vszz_results = f"./{project_name}/{project_name}_commits_introduce_vulnerabilities_advanced.json"
    # Initialize the VulnerabilityAnalyzer with file paths
    analyzer = VulnerabilityAnalyzer(project_name=project_name)

    # Generate a report based on the analysis
    results = analyzer.calculate_confusion_matrix_metrics(f"./{project_name}/{project_name}_confusion_matrix_metrics.csv", visualize_pr=True)
